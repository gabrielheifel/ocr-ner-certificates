{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff4b3f7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Instale as dependências necessárias (execute esta célula primeiro)\n",
    "\n",
    "# Atualiza os repositórios e instala Tesseract + Poppler (para PDF)\n",
    "!apt-get update\n",
    "!apt-get install -y tesseract-ocr tesseract-ocr-por poppler-utils\n",
    "\n",
    "# Instala as bibliotecas Python necessárias\n",
    "!pip install pytesseract Pillow pdf2image langdetect\n",
    "\n",
    "# Instala os modelos grandes do spaCy para português e inglês\n",
    "!python -m spacy download pt_core_news_lg\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8472f067",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS E CONFIGURAÇÕES\n",
    "# =============================================================================\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import spacy\n",
    "import json\n",
    "from typing import Dict, List\n",
    "import pandas as pd\n",
    "import re\n",
    "from langdetect import detect\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "from google.colab import drive\n",
    "\n",
    "# Monta o Google Drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "CERTIFICADOS_PATH = '/content/drive/MyDrive/Certificados'\n",
    "\n",
    "# Mapeamento único de categorias\n",
    "palavras_chave_atividades = {\n",
    "    \"Monitorias\": [\n",
    "        \"monitoria\", \"monitor\", \"monitora\", \"atividade de monitoria\",\n",
    "        \"monitoring\", \"teaching assistant\", \"mentor\", \"assistant activity\"\n",
    "    ],\n",
    "    \"Bolsista/Voluntário de Projetos de Pesquisa\": [\n",
    "        \"bolsista de pesquisa\", \"projeto de pesquisa\", \"pesquisa\", \"voluntário de pesquisa\", \"iniciação científica\",\n",
    "        \"bolsista de projeto de pesquisa\", \"voluntário de projeto de pesquisa\",\n",
    "        \"research grant\", \"research project\", \"scientific research\", \"volunteer research\", \"undergraduate research\"\n",
    "    ],\n",
    "    \"Bolsista/Voluntário de Projetos de Extensao\": [\n",
    "        \"bolsista de extensão\", \"projeto de extensão\", \"extensão universitária\", \"voluntário de extensão\",\n",
    "        \"bolsista de projeto de extensão\", \"voluntário de projeto de extensão\",\n",
    "        \"extension project\", \"university extension\", \"extension volunteer\", \"extension program\"\n",
    "    ],\n",
    "    \"Bolsista/Voluntario de Projetos de Ensino\": [\n",
    "        \"bolsista de ensino\", \"projeto de ensino\", \"ensino\", \"voluntário de ensino\",\n",
    "        \"bolsista de projeto de ensino\", \"voluntário de projeto de ensino\",\n",
    "        \"teaching project\", \"teaching volunteer\", \"teaching program\", \"educational project\"\n",
    "    ],\n",
    "    \"Participação em Atividades de Extensão (como organizador, colaborador ou ministrante)\": [\n",
    "        \"atividade de extensão\", \"organizador\", \"colaborador\", \"ministrante\", \"extensão\", \"participação em extensão\",\n",
    "        \"extension activity\", \"organizer\", \"collaborator\", \"speaker\", \"extension participation\"\n",
    "    ],\n",
    "    \"Participação em Semana Acadêmica do Curso de Computação\": [\n",
    "        \"sacomp\", \"semana acadêmica\", \"semana do curso\", \"semana de computação\", \"semana acadêmica de computação\",\n",
    "        \"academic week\", \"course week\", \"computer science week\", \"cs academic week\"\n",
    "    ],\n",
    "    \"Participação em Cursos e Escolas\": [\n",
    "        \"curso\", \"escola\", \"participação em curso\", \"participação em escola\", \"curso de\", \"escola de\",\n",
    "        \"course\", \"school\", \"participation in course\", \"participation in school\", \"course on\", \"school on\"\n",
    "    ],\n",
    "    \"Participação em Evento Científico\": [\n",
    "        \"evento científico\", \"simpósio\", \"congresso\", \"jornada\", \"encontro científico\", \"workshop\", \"seminário\", \"evento\",\n",
    "        \"scientific event\", \"symposium\", \"conference\", \"scientific meeting\", \"workshop\", \"seminar\", \"event\"\n",
    "    ],\n",
    "    \"Publicação de Artigo Científico\": [\n",
    "        \"artigo científico\", \"publicação\", \"publicado\", \"artigo\", \"paper\", \"revista científica\",\n",
    "        \"scientific article\", \"publication\", \"published\", \"article\", \"paper\", \"scientific journal\"\n",
    "    ],\n",
    "    \"Representação Estudantil\": [\n",
    "        \"representante estudantil\", \"representação estudantil\", \"diretório acadêmico\", \"centro acadêmico\", \"representante de turma\",\n",
    "        \"student representative\", \"academic representation\", \"student council\", \"class representative\"\n",
    "    ],\n",
    "    \"Obtenção de Prêmios e Distinções\": [\n",
    "        \"prêmio\", \"distinção\", \"menção honrosa\", \"premiado\", \"destaque\", \"honraria\",\n",
    "        \"award\", \"distinction\", \"honorable mention\", \"awarded\", \"highlight\", \"honor\"\n",
    "    ],\n",
    "    \"Certificações Profissionais\": [\n",
    "        \"certificação profissional\", \"certificado profissional\", \"certificação\", \"profissionalizante\",\n",
    "        \"professional certification\", \"certified professional\", \"certification\", \"professional training\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "INSTITUTION_KEYWORDS_PT = ['universidade', 'escola', 'instituto', 'faculdade', 'colégio', 'ifsul', 'ifrs', 'ifsp', 'federal', 'senac', 'senai', 'ufpel']\n",
    "INSTITUTION_KEYWORDS_EN = ['university', 'school', 'institute', 'college', 'ifsp', 'senac', 'senai', 'federal', 'rocketseat']\n",
    "ACTIONS_KEYWORDS_PT = ['curso', 'curso de', 'palestra', 'treinamento', 'oficina', 'workshop', 'formação', 'seminário', 'minicurso', 'ouvinte', 'bolsista', 'artigo', 'apresentador', 'participação', 'comissão organizadora', 'organizador', 'participou', 'prêmio', 'melhor artigo', 'melhores artigos']\n",
    "ACTIONS_KEYWORDS_EN = ['course', 'lecture', 'training', 'workshop', 'formation', 'seminar', 'mini-course', 'attendee', 'scholarship', 'article', 'presenter', 'participation', 'organizing committee', 'organizer', 'participated']\n",
    "\n",
    "print(\"✅ Configurações carregadas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5840ff",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FUNÇÕES UTILITÁRIAS\n",
    "# =============================================================================\n",
    "\n",
    "def load_json(path):\n",
    "    \"\"\"Carrega arquivo JSON\"\"\"\n",
    "    if os.path.exists(path):\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "def save_json(data, path):\n",
    "    \"\"\"Salva arquivo JSON\"\"\"\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Limpa o texto extraído\"\"\"\n",
    "    # Remove espaços extras em cada linha\n",
    "    lines = [re.sub(r'\\s+', ' ', line).strip() for line in text.splitlines()]\n",
    "    # Remove linhas totalmente vazias\n",
    "    lines = [line for line in lines if line]\n",
    "    # Junta as linhas com \\n preservando quebras de linha\n",
    "    text = '\\n'.join(lines)\n",
    "    # Remove caracteres indesejados, mantendo letras, números, pontuação e \\n\n",
    "    text = re.sub(r'[^\\w\\sÀ-ÿ.,;:!?/\\-\\n]', '', text)\n",
    "    # Corrige palavras em maiúsculas para Title Case\n",
    "    def fix_caps(match):\n",
    "        word = match.group(0)\n",
    "        return word.title()\n",
    "    text = re.sub(r'\\b[A-ZÀ-Ý]{3,}\\b', fix_caps, text)\n",
    "    return text\n",
    "\n",
    "\n",
    "print(\"✅ Funções utilitárias definidas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8307b0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FUNÇÕES DE EXTRAÇÃO DE ENTIDADES\n",
    "# =============================================================================\n",
    "\n",
    "def extract_institution_combined(tokens, text, keywords, lang='pt'):\n",
    "    \"\"\"Extrai a instituição combinando regex e entidades spaCy, escolhendo a melhor candidata\"\"\"\n",
    "\n",
    "    if lang == 'pt':\n",
    "        patterns = [\n",
    "            r'Universidade\\s+[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*',\n",
    "            r'Universidade\\s+Federal\\s+de\\s+[A-Z][a-z]+',\n",
    "            r'Instituto\\s+Federal\\s+[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*',\n",
    "            r'Faculdade\\s+[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*',\n",
    "            r'Escola\\s+[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*',\n",
    "            r'Col[eé]gio\\s+[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*',\n",
    "            r'IFSUL', r'IFRS', r'IFSP', r'SENAC', r'SENAI'\n",
    "        ]\n",
    "    else:  # English\n",
    "        patterns = [\n",
    "            r'Federal\\s+University\\s+of\\s+[A-Z][a-z]+',\n",
    "            r'University\\s+of\\s+[A-Z][a-z]+',\n",
    "            r'Institute\\s+of\\s+[A-Z][a-z]+',\n",
    "            r'College\\s+of\\s+[A-Z][a-z]+',\n",
    "            r'School\\s+of\\s+[A-Z][a-z]+',\n",
    "            r'IFSP', r'SENAC', r'SENAI', r'Rocketseat'\n",
    "        ]\n",
    "\n",
    "    candidates = []\n",
    "    found_by_regex = False\n",
    "    found_by_ner = False\n",
    "\n",
    "    # Regex - todas as matches\n",
    "    for pattern in patterns:\n",
    "        for match in re.finditer(pattern, text, re.IGNORECASE):\n",
    "            institution_found = match.group(0).strip()\n",
    "            if institution_found not in candidates:\n",
    "                candidates.append(institution_found)\n",
    "                found_by_regex = True\n",
    "\n",
    "    # Entidades spaCy com keywords\n",
    "    for ent, label in tokens['entidades']:\n",
    "        if label in ['ORG', 'LOC', 'MISC']:\n",
    "            ent_lower = ent.lower()\n",
    "            if any(kw in ent_lower for kw in keywords):\n",
    "                if ent not in candidates:\n",
    "                    candidates.append(ent)\n",
    "                    found_by_ner = True\n",
    "\n",
    "    # Função simples de score para escolher melhor candidato\n",
    "    def score(name):\n",
    "        name_lower = name.lower()\n",
    "        count_kw = sum(kw in name_lower for kw in keywords)\n",
    "        return count_kw * 10 + len(name)  # peso em keywords + comprimento\n",
    "\n",
    "    if candidates:\n",
    "        candidates.sort(key=score, reverse=True)\n",
    "        best = candidates[0]\n",
    "    else:\n",
    "        best = None\n",
    "\n",
    "    # Normalização para Udemy\n",
    "    if re.search(r'ude\\.?my', text, re.IGNORECASE):\n",
    "        best = \"Udemy\"\n",
    "\n",
    "    if best:\n",
    "        if found_by_regex and not found_by_ner:\n",
    "            print(f\"INFO: Instituição '{best}' capturada via REGEX\")\n",
    "        elif found_by_ner and not found_by_regex:\n",
    "            print(f\"INFO: Instituição '{best}' capturada via NER\")\n",
    "        elif found_by_regex and found_by_ner:\n",
    "            print(f\"INFO: Instituição '{best}' capturada via REGEX e NER\")\n",
    "        else:\n",
    "            print(f\"INFO: Instituição '{best}' capturada, origem desconhecida\")\n",
    "\n",
    "    return best\n",
    "\n",
    "def extract_action_combined(tokens, text, keywords, lang='pt'):\n",
    "    candidates = []\n",
    "    found_by_regex = False\n",
    "    found_by_ner = False\n",
    "\n",
    "    # Padrões simples de regex para capturar expressões comuns de ação\n",
    "    patterns = []\n",
    "    if lang == 'pt':\n",
    "        patterns = [rf'{kw}[^.,;]*' for kw in keywords if len(kw) > 2]  # evita keywords muito curtas\n",
    "    else:\n",
    "        patterns = [rf'{kw}[^.,;]*' for kw in keywords if len(kw) > 2]\n",
    "\n",
    "    # Busca candidatos por regex\n",
    "    for pattern in patterns:\n",
    "        for match in re.finditer(pattern, text, re.IGNORECASE):\n",
    "            candidate = match.group(0).strip()\n",
    "            if candidate not in candidates:\n",
    "                candidates.append(candidate)\n",
    "                found_by_regex = True\n",
    "\n",
    "    # Busca em entidades NER com labels plausíveis para ações\n",
    "    ner_labels = ['MISC', 'WORK_OF_ART', 'EVENT', 'ORG']\n",
    "    for ent, label in tokens['entidades']:\n",
    "        if label in ner_labels:\n",
    "            ent_lower = ent.lower()\n",
    "            if any(kw in ent_lower for kw in keywords):\n",
    "                if ent not in candidates:\n",
    "                    candidates.append(ent)\n",
    "                    found_by_ner = True\n",
    "\n",
    "    # Score simples para escolher melhor candidato: peso para mais keywords + tamanho\n",
    "    def score(name):\n",
    "        name_lower = name.lower()\n",
    "        count_kw = sum(kw in name_lower for kw in keywords)\n",
    "        return count_kw * 10 + len(name)\n",
    "\n",
    "    if candidates:\n",
    "        candidates.sort(key=score, reverse=True)\n",
    "        best = candidates[0]\n",
    "\n",
    "        # Print de origem da detecção\n",
    "        if found_by_regex and not found_by_ner:\n",
    "            print(f\"INFO: Ação '{best}' capturada via REGEX\")\n",
    "        elif found_by_ner and not found_by_regex:\n",
    "            print(f\"INFO: Ação '{best}' capturada via NER\")\n",
    "        elif found_by_regex and found_by_ner:\n",
    "            print(f\"INFO: Ação '{best}' capturada via REGEX e NER\")\n",
    "        else:\n",
    "            print(f\"INFO: Ação '{best}' capturada, origem desconhecida\")\n",
    "\n",
    "        return best\n",
    "\n",
    "    return None\n",
    "\n",
    "def is_valid_name(name: str) -> bool:\n",
    "    words = name.strip().split()\n",
    "    if len(words) < 2:\n",
    "        return False\n",
    "    if any(not w[0].isupper() for w in words):\n",
    "        return False\n",
    "    if any(any(c.isdigit() for c in w) for w in words):\n",
    "        return False\n",
    "    invalid_words = {'Prof', 'Dr', 'Sr', 'Sra', 'Ms'}\n",
    "    if any(w in invalid_words for w in words):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def extract_name_with_context(tokens):\n",
    "    \"\"\"Extrai nome usando contexto e filtrando por palavras-chave válidas e inválidas.\"\"\"\n",
    "    context_keywords = ['certificamos que', 'aluno', 'participou', 'concluiu']\n",
    "    invalid_keywords = ['coordenador', 'professor', 'coordinator', 'director', 'instructor']\n",
    "\n",
    "    name_candidates = []\n",
    "\n",
    "    for sentence in tokens['frases']:\n",
    "        sentence_lower = sentence.lower()\n",
    "\n",
    "        # Pular frases com cargos inválidos\n",
    "        if any(invalid in sentence_lower for invalid in invalid_keywords):\n",
    "            continue\n",
    "\n",
    "        # Verificar se a frase contém alguma palavra-chave de contexto\n",
    "        if any(keyword in sentence_lower for keyword in context_keywords):\n",
    "            for ent, label in tokens['entidades']:\n",
    "                if label == 'PER' and ent in sentence:\n",
    "                    name_candidates.append(ent)\n",
    "\n",
    "    if name_candidates:\n",
    "        return name_candidates[0]\n",
    "    return None\n",
    "\n",
    "def extract_name_with_context_en(tokens):\n",
    "    \"\"\"Extracts person name using context keywords in English\"\"\"\n",
    "    context_keywords = ['certify that', 'student', 'attended', 'completed', 'participated']\n",
    "    invalid_keywords = ['coordinator','professor','director','instructor','manager','supervisor','teacher','head']\n",
    "\n",
    "    name_candidates = []\n",
    "\n",
    "    for sentence in tokens['frases']:\n",
    "        sentence_lower = sentence.lower()\n",
    "\n",
    "        # Pular frases com cargos inválidos\n",
    "        if any(invalid in sentence_lower for invalid in invalid_keywords):\n",
    "            continue\n",
    "\n",
    "        # Verificar se a frase contém alguma palavra-chave de contexto\n",
    "        if any(keyword in sentence_lower for keyword in context_keywords):\n",
    "            for ent, label in tokens['entidades']:\n",
    "                if label == 'PERSON' and ent in sentence:\n",
    "                    name_candidates.append(ent)\n",
    "\n",
    "    if name_candidates:\n",
    "        return name_candidates[0]\n",
    "    return None\n",
    "\n",
    "def extract_user_name(tokens, lang):\n",
    "    if lang == 'pt':\n",
    "        name_context = extract_name_with_context(tokens)\n",
    "        if name_context and is_valid_name(name_context):\n",
    "            return name_context\n",
    "        else:\n",
    "            for ent, label in tokens['entidades']:\n",
    "                if label == 'PER' and is_valid_name(ent):\n",
    "                    return ent\n",
    "    else:  # inglês\n",
    "        name_context = extract_name_with_context_en(tokens)\n",
    "        if name_context and is_valid_name(name_context):\n",
    "            return name_context\n",
    "        else:\n",
    "            for ent, label in tokens['entidades']:\n",
    "                if label == 'PERSON' and is_valid_name(ent):\n",
    "                    return ent\n",
    "    return None\n",
    "\n",
    "print(\"✅ Funções de extração definidas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37051031",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FUNÇÕES DE PROCESSAMENTO PRINCIPAL\n",
    "# =============================================================================\n",
    "\n",
    "def load_spacy_models():\n",
    "    \"\"\"Carrega modelos spaCy\"\"\"\n",
    "    try:\n",
    "        nlp_pt = spacy.load('pt_core_news_lg')\n",
    "    except OSError:\n",
    "        print(\"Baixando modelo de linguagem em português...\")\n",
    "        print(\"Execute: !python -m spacy download pt_core_news_lg\")\n",
    "        print(\"\\n⚠️ IMPORTANTE: Por favor, reinicie o kernel/runtime e execute o código novamente!\")\n",
    "        raise Exception(\"Reinicie o kernel para carregar o modelo spaCy\")\n",
    "    try:\n",
    "        nlp_en = spacy.load('en_core_web_lg')\n",
    "    except OSError:\n",
    "        print(\"Baixando modelo de linguagem em inglês...\")\n",
    "        print(\"Execute: !python -m spacy download en_core_web_lg\")\n",
    "        print(\"\\n⚠️ IMPORTANTE: Por favor, reinicie o kernel/runtime e execute o código novamente!\")\n",
    "        raise Exception(\"Reinicie o kernel para carregar o modelo spaCy em inglês\")\n",
    "    print(\"✅ Modelos spaCy carregados!\")\n",
    "    return nlp_pt, nlp_en\n",
    "\n",
    "def process_text(nlp, text: str) -> Dict:\n",
    "    \"\"\"Processa texto com spaCy\"\"\"\n",
    "    doc = nlp(text)\n",
    "    tokens = {\n",
    "        'entidades': [(ent.text, ent.label_) for ent in doc.ents],\n",
    "        'frases': [sent.text for sent in doc.sents],\n",
    "        'palavras_chave': [token.text for token in doc if not token.is_stop and not token.is_punct],\n",
    "        'original_text': text\n",
    "    }\n",
    "    print(tokens)\n",
    "    # Print the size of the 'entidades' array\n",
    "    print(f\"Tamanho do array 'entidades': {len(tokens['entidades'])}\")\n",
    "    return tokens\n",
    "\n",
    "def extract_info_pt(tokens):\n",
    "    \"\"\"Extrai informações de texto em português\"\"\"\n",
    "    result = {\n",
    "        'user_name': None,\n",
    "        'action': None,\n",
    "        'institution': None,\n",
    "        'dates': [],\n",
    "        'hours': [],\n",
    "        'detected_categories': []\n",
    "    }\n",
    "\n",
    "    # Name extraction with context\n",
    "    result['user_name'] = extract_user_name(tokens, lang='pt')\n",
    "\n",
    "    # Institution extraction with regex and context\n",
    "    result['institution'] = extract_institution_combined(tokens, tokens['original_text'], INSTITUTION_KEYWORDS_PT, lang='pt')\n",
    "\n",
    "    # Action extraction\n",
    "    result['action'] = extract_action_combined(tokens, tokens['original_text'], ACTIONS_KEYWORDS_PT, lang='pt')\n",
    "\n",
    "    # Date and Hour extraction\n",
    "    text = tokens['original_text']\n",
    "    for ent, label in tokens['entidades']:\n",
    "        if label == 'DATE' and ent not in result['dates']:\n",
    "            result['dates'].append(ent)\n",
    "\n",
    "    # Regex para datas específicas e formatos\n",
    "    date_pattern = (\n",
    "        r'\\b\\d{1,2}[./]\\d{1,2}[./]\\d{2,4}\\b'\n",
    "        r'|'\n",
    "        r'de \\d{1,2} a \\d{1,2} de [a-zçãé]+ de \\d{4}'\n",
    "        r'|'\n",
    "        r'entre \\d{1,2} e \\d{1,2} de [a-zçãé]+ de \\d{4}'\n",
    "        r'|'\n",
    "        r'\\d{1,2} a \\d{1,2} de [a-zçãé]+ de \\d{4}'\n",
    "        r'|'\n",
    "        r'\\d{1,2} de [a-zçãé]+ de \\d{4}'\n",
    "    )\n",
    "    dates_regex = re.findall(date_pattern, text, re.IGNORECASE)\n",
    "    for date_str in dates_regex:\n",
    "        if date_str not in result['dates']:\n",
    "            # Processa intervalos para separar em duas datas\n",
    "            match = re.match(r'(?:de )?(\\d{1,2}) a (\\d{1,2}) de ([a-zçãé]+) de (\\d{4})', date_str, re.IGNORECASE)\n",
    "            if match:\n",
    "                day_start, day_end, month, year = match.groups()\n",
    "                d1 = f\"{int(day_start):02d} de {month} de {year}\"\n",
    "                d2 = f\"{int(day_end):02d} de {month} de {year}\"\n",
    "                if d1 not in result['dates']:\n",
    "                    result['dates'].append(d1)\n",
    "                if d2 not in result['dates']:\n",
    "                    result['dates'].append(d2)\n",
    "            else:\n",
    "                result['dates'].append(date_str)\n",
    "\n",
    "    for ent, label in tokens['entidades']:\n",
    "        if label == 'TIME' and ent not in result['hours']:\n",
    "            result['hours'].append(ent)\n",
    "\n",
    "    # Regex para horas\n",
    "    hours_regex = re.findall(r'\\b\\d{1,5}\\s?(?:h|horas?|hora\\(s\\)?)\\b', text, re.IGNORECASE)\n",
    "    hours_hhmm = re.findall(r'\\b\\d{1,5}:00\\b', text)\n",
    "    for h in hours_regex + hours_hhmm:\n",
    "        if h not in result['hours']:\n",
    "            result['hours'].append(h)\n",
    "\n",
    "    # Category detection\n",
    "    text_lower = text.lower()\n",
    "    detected_categories = set()\n",
    "    if 'ouvinte' in text_lower:\n",
    "        detected_categories.add('ouvinte')\n",
    "    else:\n",
    "        for category, keywords in palavras_chave_atividades.items():\n",
    "            for keyword in keywords:\n",
    "                if keyword.lower() in text_lower:\n",
    "                    detected_categories.add(category)\n",
    "                    break\n",
    "    result['detected_categories'] = list(detected_categories)\n",
    "\n",
    "    return result\n",
    "\n",
    "def extract_info_en(tokens):\n",
    "    \"\"\"Extrai informações de texto em inglês\"\"\"\n",
    "    result = {\n",
    "        'user_name': None,\n",
    "        'action': None,\n",
    "        'institution': None,\n",
    "        'dates': [],\n",
    "        'hours': [],\n",
    "        'detected_categories': []\n",
    "    }\n",
    "\n",
    "    # Name extraction\n",
    "    result['user_name'] = extract_user_name(tokens, lang='en')\n",
    "\n",
    "    # Institution extraction\n",
    "    result['institution'] = extract_institution_combined(tokens, tokens['original_text'], INSTITUTION_KEYWORDS_EN, lang='en')\n",
    "\n",
    "    # Action extraction\n",
    "    result['action'] = extract_action_combined(tokens, tokens['original_text'], ACTIONS_KEYWORDS_EN, lang='en')\n",
    "\n",
    "    # Dates and hours extraction\n",
    "    text = tokens['original_text']\n",
    "\n",
    "    # Datas de NER\n",
    "    for ent, label in tokens['entidades']:\n",
    "        if label == 'DATE' and ent not in result['dates']:\n",
    "            result['dates'].append(ent)\n",
    "\n",
    "    # Datas regex\n",
    "    date_pattern = (\n",
    "        r'\\b\\d{1,2}[./]\\d{1,2}[./]\\d{2,4}\\b'\n",
    "        r'|'\n",
    "        r'from \\d{1,2} to \\d{1,2} [a-z]+ \\d{4}'\n",
    "        r'|'\n",
    "        r'between \\d{1,2} and \\d{1,2} [a-z]+ \\d{4}'\n",
    "        r'|'\n",
    "        r'\\d{1,2} to \\d{1,2} [a-z]+ \\d{4}'\n",
    "        r'|'\n",
    "        r'\\d{1,2} [a-z]+ \\d{4}'\n",
    "    )\n",
    "    dates_regex = re.findall(date_pattern, text, re.IGNORECASE)\n",
    "    for d in dates_regex:\n",
    "        if d not in result['dates']:\n",
    "            result['dates'].append(d)\n",
    "\n",
    "    # Horas NER\n",
    "    for ent, label in tokens['entidades']:\n",
    "        if label == 'TIME' and ent not in result['hours']:\n",
    "            result['hours'].append(ent)\n",
    "\n",
    "    # Horas regex\n",
    "    hours_regex = re.findall(r'\\b\\d{1,5}\\s?(?:h|hours?)\\b', text, re.IGNORECASE)\n",
    "    hours_hhmm = re.findall(r'\\b\\d{1,5}:00\\b', text)\n",
    "    for h in hours_regex + hours_hhmm:\n",
    "        if h not in result['hours']:\n",
    "            result['hours'].append(h)\n",
    "\n",
    "    # Category detection\n",
    "    text_lower = text.lower()\n",
    "    detected_categories = set()\n",
    "    if 'listener' in text_lower:\n",
    "        detected_categories.add('ouvinte')\n",
    "    else:\n",
    "        for category, keywords in palavras_chave_atividades.items():\n",
    "            for keyword in keywords:\n",
    "                if keyword.lower() in text_lower:\n",
    "                    detected_categories.add(category)\n",
    "                    break\n",
    "    result['detected_categories'] = list(detected_categories)\n",
    "\n",
    "    return result\n",
    "\n",
    "print(\"✅ Funções de processamento definidas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58af3449",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FUNÇÃO PRINCIPAL DE PROCESSAMENTO\n",
    "# =============================================================================\n",
    "\n",
    "def process_json_file(nlp_pt, nlp_en, file_path: str) -> Dict:\n",
    "    \"\"\"Processa um arquivo JSON contendo um array de objetos com campo 'text'\"\"\"\n",
    "    resultados = {}\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            conteudo = f.read()\n",
    "            data = json.loads(conteudo)\n",
    "\n",
    "        if not isinstance(data, dict) or 'processedFiles' not in data:\n",
    "            print(f\"Erro: O arquivo não contém o campo 'processedFiles'. Campos disponíveis: {list(data.keys())}\")\n",
    "            return {}\n",
    "\n",
    "        processed_files = data['processedFiles']\n",
    "        if not isinstance(processed_files, list):\n",
    "            print(f\"Erro: O campo 'processedFiles' não é um array. Tipo encontrado: {type(processed_files)}\")\n",
    "            return {}\n",
    "\n",
    "        encontrou_texto = False\n",
    "\n",
    "        for i, item in enumerate(processed_files):\n",
    "            if 'text' in item and item['text']:\n",
    "                encontrou_texto = True\n",
    "                print(f\"\\nProcessing text from file: {item['fileName']}\")\n",
    "                print(\"\\nOriginal text:\")\n",
    "                print(\"-\" * 50)\n",
    "                print(item['text'])\n",
    "                print(\"-\" * 50)\n",
    "\n",
    "                # Clean the text before processing\n",
    "                cleaned_text = clean_text(item['text'])\n",
    "\n",
    "                # Detect language using langdetect\n",
    "                try:\n",
    "                    language = detect(cleaned_text)\n",
    "                except Exception:\n",
    "                    language = 'pt'  # fallback\n",
    "\n",
    "                if language == 'en':\n",
    "                    #print(\"Detected language: English (en). Using spaCy English model.\")\n",
    "                    tokens = process_text(nlp_en, cleaned_text)\n",
    "                    info_extracted = extract_info_en(tokens)\n",
    "                else:\n",
    "                    #print(f\"Detected language: {language}. Using spaCy Portuguese model.\")\n",
    "                    tokens = process_text(nlp_pt, cleaned_text)\n",
    "                    info_extracted = extract_info_pt(tokens)\n",
    "\n",
    "                # Adiciona o texto limpo ao dicionário de resultados\n",
    "                info_extracted['cleaned_text'] = cleaned_text\n",
    "\n",
    "                # Adiciona o link ao arquivo processado\n",
    "                info_extracted['file_link'] = f\"drive.com/{item['fileName']}\"\n",
    "\n",
    "                print(\"\\nExtracted information from this file:\")\n",
    "                print(\"-\" * 50)\n",
    "                for key, value in info_extracted.items():\n",
    "                    print(f\"{key}: {value}\")\n",
    "\n",
    "                resultados[item['fileName']] = info_extracted\n",
    "\n",
    "        if not encontrou_texto:\n",
    "            print(\"Nenhum texto válido encontrado para processar\")\n",
    "\n",
    "        return resultados\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Erro ao decodificar JSON: {e}\")\n",
    "        print(\"Verifique se o arquivo está em formato JSON válido\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar arquivo {file_path}: {e}\")\n",
    "        return {}\n",
    "\n",
    "def save_results(results: Dict, output_path: str):\n",
    "    \"\"\"Salva os resultados em um arquivo JSON\"\"\"\n",
    "    try:\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Resultados salvos em: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao salvar resultados: {e}\")\n",
    "        print(\"Verifique se você tem permissão para escrever no diretório.\")\n",
    "\n",
    "def extract_text_with_pytesseract(folder_path):\n",
    "    \"\"\"Extrai texto de imagens/PDFs usando pytesseract\"\"\"\n",
    "    processed_files = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        ext = file_name.lower().split('.')[-1] if '.' in file_name else ''\n",
    "        try:\n",
    "            if ext in ('png', 'jpg', 'jpeg', 'tiff'):\n",
    "                img = Image.open(file_path)\n",
    "                text = pytesseract.image_to_string(img, lang='por')\n",
    "                processed_files.append({\n",
    "                    \"fileName\": file_name,\n",
    "                    \"type\": ext,\n",
    "                    \"text\": text\n",
    "                })\n",
    "            elif ext == 'pdf':\n",
    "                pages = convert_from_path(file_path)\n",
    "                text = \"\"\n",
    "                for page in pages:\n",
    "                    text += pytesseract.image_to_string(page, lang='por') + \"\\n\"\n",
    "                processed_files.append({\n",
    "                    \"fileName\": file_name,\n",
    "                    \"type\": ext,\n",
    "                    \"text\": text\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar {file_name}: {e}\")\n",
    "    return {\n",
    "        \"totalFiles\": len(processed_files),\n",
    "        \"processedFiles\": processed_files\n",
    "    }\n",
    "\n",
    "print(\"✅ Função principal de processamento definida!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce1d7e4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FUNÇÃO CRIAR RELATÓRIOS\n",
    "# =============================================================================\n",
    "\n",
    "# Mapeamento de categorias resumidas\n",
    "CATEGORIAS_PRINCIPAIS = {\n",
    "    'Ensino': [\n",
    "        \"Monitorias\",\n",
    "        \"Bolsista/Voluntario de Projetos de Ensino\"\n",
    "    ],\n",
    "    'Pesquisa': [\n",
    "        \"Bolsista/Voluntário de Projetos de Pesquisa\",\n",
    "        \"Publicação de Artigo Científico\"\n",
    "    ],\n",
    "    'Extensão': [\n",
    "        \"Bolsista/Voluntário de Projetos de Extensao\",\n",
    "        \"Participação em Atividades de Extensão (como organizador, colaborador ou ministrante)\",\n",
    "        \"Participação em Semana Acadêmica do Curso de Computação\",\n",
    "        \"Participação em Evento Científico\",\n",
    "        \"Representação Estudantil\",\n",
    "        \"Obtenção de Prêmios e Distinções\",\n",
    "        \"Certificações Profissionais\"\n",
    "    ],\n",
    "    'Livres': [\n",
    "        \"Participação em Cursos e Escolas\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "def normalize_name(name: str) -> str:\n",
    "    \"\"\"Simplifica nome para comparação: minúsculo e sem espaços extras\"\"\"\n",
    "    return ' '.join(name.strip().lower().split())\n",
    "\n",
    "def parse_hours(hours_list):\n",
    "    \"\"\"Converte lista de strings de horas para float total (horas)\"\"\"\n",
    "    total = 0.0\n",
    "    for h in hours_list:\n",
    "        h = h.lower().strip()\n",
    "        if ':' in h:\n",
    "            parts = h.split(':')\n",
    "            if len(parts) == 2 and parts[0].isdigit() and parts[1].isdigit():\n",
    "                total += int(parts[0]) + int(parts[1])/60\n",
    "        else:\n",
    "            match = re.search(r'(\\d+(?:[\\.,]\\d+)?)', h)\n",
    "            if match:\n",
    "                val = match.group(1).replace(',', '.')\n",
    "                try:\n",
    "                    total += float(val)\n",
    "                except:\n",
    "                    pass\n",
    "    return total\n",
    "\n",
    "def categorize_hours(cert):\n",
    "    \"\"\"Recebe um certificado e retorna um dicionário com horas por categoria principal\"\"\"\n",
    "    horas_total = parse_hours(cert.get('hours', []))\n",
    "    categorias_detected = cert.get('detected_categories', [])\n",
    "\n",
    "    horas_por_categoria = {\n",
    "        'Ensino': 0.0,\n",
    "        'Pesquisa': 0.0,\n",
    "        'Extensão': 0.0,\n",
    "        'Livres': 0.0\n",
    "    }\n",
    "\n",
    "    for cat in categorias_detected:\n",
    "        # Padronizações especiais por categoria\n",
    "        if cat == \"Participação em Evento Científico\":\n",
    "            horas_adicionais = 17.0\n",
    "            horas_por_categoria['Extensão'] += horas_adicionais\n",
    "        elif cat == \"Publicação de Artigo Científico\":\n",
    "            horas_adicionais = 34.0\n",
    "            horas_por_categoria['Pesquisa'] += horas_adicionais\n",
    "        elif cat == \"Obtenção de Prêmios e Distinções\":\n",
    "            horas_adicionais = 68.0\n",
    "            horas_por_categoria['Extensão'] += horas_adicionais\n",
    "        else:\n",
    "            # Para categorias normais, adiciona o total extraído\n",
    "            for main_cat, subcats in CATEGORIAS_PRINCIPAIS.items():\n",
    "                if cat in subcats:\n",
    "                    horas_por_categoria[main_cat] += horas_total\n",
    "                    break\n",
    "\n",
    "    # Se não tiver categoria, considera como Formação Livre\n",
    "    if sum(horas_por_categoria.values()) == 0:\n",
    "        horas_por_categoria['Livres'] += horas_total\n",
    "\n",
    "    return horas_por_categoria\n",
    "\n",
    "def aggregate_certificates(certificates):\n",
    "    aggregation = {}\n",
    "    for cert in certificates:\n",
    "        name = cert.get('user_name')\n",
    "        if not name:\n",
    "            continue\n",
    "        norm_name = normalize_name(name)\n",
    "\n",
    "        horas_cat = categorize_hours(cert)\n",
    "\n",
    "        if norm_name not in aggregation:\n",
    "            aggregation[norm_name] = {\n",
    "                'name': name,\n",
    "                'total_hours': 0.0,\n",
    "                'category_hours': {'Ensino':0.0,'Pesquisa':0.0,'Extensão':0.0,'Livres':0.0},\n",
    "                'certificates': []\n",
    "            }\n",
    "\n",
    "        aggregation[norm_name]['total_hours'] += sum(horas_cat.values())\n",
    "        for key in horas_cat:\n",
    "            aggregation[norm_name]['category_hours'][key] += horas_cat[key]\n",
    "\n",
    "        aggregation[norm_name]['certificates'].append(cert)\n",
    "\n",
    "    return aggregation\n",
    "\n",
    "\n",
    "def save_report_txt(aggregation, output_path):\n",
    "    \"\"\"Salva relatório de certificados em formato de texto (ordem alfabética)\"\"\"\n",
    "    try:\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            # Ordena pelo nome normalizado\n",
    "            for norm_name, data in sorted(aggregation.items(), key=lambda x: x[0]):\n",
    "                ensino = data['category_hours'].get('Ensino', 0.0)\n",
    "                pesquisa = data['category_hours'].get('Pesquisa', 0.0)\n",
    "                extensao = data['category_hours'].get('Extensão', 0.0)\n",
    "                livres_lancada = data['category_hours'].get('Livres', 0.0)\n",
    "\n",
    "                limite_atividades_complementar = 320\n",
    "                limite_formacao_livre = 217\n",
    "\n",
    "                total_complementares = ensino + pesquisa + extensao\n",
    "                aproveitadas_complementares = min(total_complementares, limite_atividades_complementar)\n",
    "                excedente_para_livre = max(total_complementares - limite_atividades_complementar, 0)\n",
    "                total_formacao_livre = livres_lancada + excedente_para_livre\n",
    "\n",
    "                # Análise\n",
    "                analise = []\n",
    "                if aproveitadas_complementares >= limite_atividades_complementar:\n",
    "                    analise.append(\"A carga horária necessária em Atividades Complementares foi obtida.\")\n",
    "                else:\n",
    "                    analise.append(\"Ainda faltam horas em Atividades Complementares.\")\n",
    "\n",
    "                if total_formacao_livre >= limite_formacao_livre:\n",
    "                    analise.append(\"A carga horária necessária em Formação Livre foi obtida.\")\n",
    "                else:\n",
    "                    analise.append(\"Ainda faltam horas em Formação Livre.\")\n",
    "\n",
    "                # Pelo menos 2 categorias nas complementares\n",
    "                categorias_com_horas = sum(1 for v in [ensino, pesquisa, extensao] if v > 0)\n",
    "                if categorias_com_horas < 2:\n",
    "                    analise.append(\"O aluno precisa ter horas em pelo menos duas categorias de Atividades Complementares.\")\n",
    "\n",
    "                # Relatório\n",
    "                f.write(f\"Extrato de Atividades Complementares\\n\")\n",
    "                f.write(f\"Nome: {data['name']}\\n\\n\")\n",
    "                f.write(f\"Total de horas: {data['total_hours']:.2f}h\\n\")\n",
    "                f.write(f\"Ensino: {ensino:.2f}h\\n\")\n",
    "                f.write(f\"Pesquisa: {pesquisa:.2f}h\\n\")\n",
    "                f.write(f\"Extensão: {extensao:.2f}h\\n\\n\")\n",
    "                f.write(f\"Total em Atividades Complementares: {total_complementares:.2f}h\\n\")\n",
    "                f.write(f\"Aproveitadas em Atividades Complementares: {aproveitadas_complementares:.2f}h\\n\\n\")\n",
    "                f.write(f\"Formação Livre (aproveitada da complementar): {excedente_para_livre:.2f}h\\n\")\n",
    "                f.write(f\"Formação Livre (lançada): {livres_lancada:.2f}h\\n\")\n",
    "                f.write(f\"Total em Formação Livre: {total_formacao_livre:.2f}h\\n\\n\")\n",
    "                f.write(\"**Análise:**\\n\" + \" \".join(analise) + \"\\n\\n\")\n",
    "                f.write(\"=============================================================================\\n\\n\")\n",
    "        print(f\"Relatório de texto salvo em: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao salvar relatório: {e}\")\n",
    "\n",
    "print(\"✅ Função relatórios definida!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d14aa73",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FUNÇÃO MAIN E EXECUÇÃO\n",
    "# =============================================================================\n",
    "\n",
    "import time\n",
    "\n",
    "def main():\n",
    "    \"\"\"Função principal\"\"\"\n",
    "    nlp_pt, nlp_en = load_spacy_models()\n",
    "    certificados_path = CERTIFICADOS_PATH\n",
    "    json_file_path = os.path.join(certificados_path, 'Untitled')\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        tesseract_version = subprocess.check_output(['tesseract', '--version']).decode('utf-8').splitlines()[0]\n",
    "    except Exception:\n",
    "        tesseract_version = \"desconhecida\"\n",
    "    print(f\"\\nExtraindo textos dos certificados com pytesseract na versão {tesseract_version}...\")\n",
    "    data = extract_text_with_pytesseract(certificados_path)\n",
    "    temp_json_path = os.path.join(certificados_path, 'temp_processed.json')\n",
    "    with open(temp_json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    results = process_json_file(nlp_pt, nlp_en, temp_json_path)\n",
    "    if results:\n",
    "        output_path = '/content/drive/MyDrive/resultados_processamento.json'\n",
    "        save_results(results, output_path)\n",
    "\n",
    "        agrupados = aggregate_certificates(results.values())\n",
    "        output_path = '/content/drive/MyDrive/relatorios.txt'\n",
    "        save_report_txt(agrupados, output_path)\n",
    "        print(\"✅ Relatório agregado salvo em relatorios.txt\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time # Calculate the elapsed time\n",
    "    print(f\"\\nTempo total para gerar resultados: {elapsed_time:.2f} segundos\") # Print the elapsed time\n",
    "\n",
    "\n",
    "print(\"✅ Função main definida!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c00c169",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
